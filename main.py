import logging
import requests
import time
from aiogram import Bot, Dispatcher, types
from aiogram.contrib.middlewares.logging import LoggingMiddleware
from aiogram import executor
from queue import Queue
from threading import Thread
import io

API_TOKEN = '7110189993:AAHZa7DM6dak3XVrhS0Z0R6GFe5KdKWK2zQ'  # –¢–æ–∫–µ–Ω —Ç–≤–æ–µ–≥–æ –±–æ—Ç–∞

# –°–ø–∏—Å–æ–∫ API-–∫–ª—é—á–µ–π Gemini (7 —Ç–æ–∫–µ–Ω–æ–≤)
API_GEMINI_KEYS = [
    "AIzaSyDxzS_twSmnF9beT9v94SmEwH9EP7NjBHo", 
    "AIzaSyCE9bsPQXgveliU9eyffR1YOL_z5QEWT2E", 
    "AIzaSyAYAQ-8SxkwGVGMPozlQG4fTaFpDeRMADM", 
    "AIzaSyBzfQCDChMv5YcdAKcEmdCXr4-EztJ8IOk", 
    "AIzaSyDaiI_pKXdKksQD-PCJIRkYZxPM9wqqhgs", 
    "AIzaSyBrnujY135GhloN1QtHj4j8Cct6F6dRd4g", 
    "AIzaSyBKz-XkNxGDOd-oCzxIvzL0Rcn-lqDG1u8"
]  # –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ —Å–≤–æ–∏ API-–∫–ª—é—á–∏

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)

# –°–æ–∑–¥–∞–Ω–∏–µ –±–æ—Ç–∞ –∏ –¥–∏—Å–ø–µ—Ç—á–µ—Ä–∞
bot = Bot(token=API_TOKEN)
dp = Dispatcher(bot)
dp.middleware.setup(LoggingMiddleware())

# –û—á–µ—Ä–µ–¥—å –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ (—Ä–∞–∑–º–µ—Ä –æ—á–µ—Ä–µ–¥–∏ - 100 –∑–∞–ø—Ä–æ—Å–æ–≤)
request_queue = Queue(maxsize=100)

# –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
context = {}

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
def process_requests():
    while True:
        prompt = request_queue.get()
        try:
            user_id = prompt['user_id']  # –ü–æ–ª—É—á–∞–µ–º ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
            request = prompt['request']
            # –ü–æ–ø—ã—Ç–∫–∞ –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç Gemini —Å –ø–æ–≤—Ç–æ—Ä–æ–º –∑–∞–ø—Ä–æ—Å–∞
            response_text = get_llm_response(request, user_id)
            request_queue.task_done()  # –û—Ç–º–µ—Ç–∫–∞ –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞: {e}")
            request_queue.task_done()

# –ó–∞–ø—É—Å–∫ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ—Ç–æ–∫–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ - 7)
for _ in range(7):
    thread = Thread(target=process_requests)
    thread.daemon = True
    thread.start()

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç –º–æ–¥–µ–ª–∏ Gemini —Å –ø–æ–≤—Ç–æ—Ä–æ–º –∑–∞–ø—Ä–æ—Å–∞
def get_llm_response(prompt, user_id):
    max_retries = 5  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–æ–≤
    retry_count = 0
    while retry_count < max_retries:
        for api_key in API_GEMINI_KEYS:
            try:
                with open('prompt.txt', 'r', encoding='utf-8') as file:
                    prompt_content = file.read()
                prompt_with_customization = prompt_content + str(prompt)
                headers = {"Content-Type": "application/json"}
                params = {"key": api_key}
                json_data = {
                    'contents': [
                        {
                            'parts': [{"text": prompt_with_customization}],
                        },
                    ],
                    'generationConfig': {
                        #  'temperature': PRO_TEMPERATURE,
                        # 'topK': 1,
                        # 'topP': 1,
                        'maxOutputTokens': 15000,  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–æ 15000
                        # 'stopSequences': [],
                    },
                    'safetySettings': [
                        {
                            'category': 'HARM_CATEGORY_HARASSMENT',
                            'threshold': 'block_none',
                        },
                        {
                            'category': 'HARM_CATEGORY_HATE_SPEECH',
                            'threshold': 'block_none',
                        },
                        {
                            'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT',
                            'threshold': 'block_none',
                        },
                        {
                            'category': 'HARM_CATEGORY_DANGEROUS_CONTENT',
                            'threshold': 'block_none',
                        },
                    ],
                }
                response = requests.post(
                    'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent',
                    params=params,
                    headers=headers,
                    json=json_data,
                )
                response.raise_for_status()  # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–¥–∞ –æ—Ç–≤–µ—Ç–∞
                return response.json()['candidates'][0]['content']['parts'][0]['text']
            except requests.exceptions.RequestException as e:
                logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ –∑–∞–ø—Ä–æ—Å–∞ –∫ API Gemini: {e}")
                continue  # –ü–µ—Ä–µ—Ö–æ–¥ –∫ —Å–ª–µ–¥—É—é—â–µ–º—É API-–∫–ª—é—á—É
        retry_count += 1
        time.sleep(1)  # –ü–∞—É–∑–∞ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–æ–π
    raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç –≤—Å–µ—Ö API Gemini")

# –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /start
@dp.message_handler(commands=['start'])
async def send_welcome(message: types.Message):
    await message.answer("*üëã –î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å –≤ KingApk | Gemini Pro 1.5!*\n\n*‚ùì –ó–∞–¥–∞–π—Ç–µ —Å–≤–æ–π –≤–æ–ø—Ä–æ—Å —á—Ç–æ–±—ã —è —Å–º–æ–≥ –Ω–∞ –Ω–µ–≥–æ –æ—Ç–≤–µ—Ç–∏—Ç—å.*\n\n_ * - –µ—Å–ª–∏ –±–æ—Ç –¥–æ–ª–≥–æ –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–∞—à –∑–∞–ø—Ä–æ—Å - –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –µ–≥–æ :(_", parse_mode="Markdown")

# –û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å–æ–æ–±—â–µ–Ω–∏–π
@dp.message_handler()
async def echo(message: types.Message):
    prompt = message.text
    user_id = message.from_user.id

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    if user_id not in context:
        context[user_id] = ""

    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –≤ –æ—á–µ—Ä–µ–¥—å —Å ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    try:
        request_queue.put({'user_id': user_id, 'request': prompt}, block=False)  
    except queue.Full:
        await message.reply("–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –±–æ—Ç –ø–µ—Ä–µ–≥—Ä—É–∂–µ–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.", parse_mode="Markdown")
        return  # –í—ã—Ö–æ–¥ –∏–∑ —Ñ—É–Ω–∫—Ü–∏–∏

    await bot.send_chat_action(message.chat.id, types.ChatActions.TYPING)

    # –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞
    await request_queue.join()

    # –ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –∏–∑ –æ—á–µ—Ä–µ–¥–∏
    try:
        response_text = request_queue.get_nowait()
        if len(response_text) > 2000:  # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã –æ—Ç–≤–µ—Ç–∞
            # –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ —Å –æ—Ç–≤–µ—Ç–æ–º
            with io.BytesIO(response_text.encode('utf-8')) as file:
                file.name = "–æ—Ç–≤–µ—Ç.txt"
                await message.reply_document(file)
        else:
            await message.reply(response_text, parse_mode="Markdown")

        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        context[user_id] = response_text  # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ—Ç–≤–µ—Ç–∞ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
    except queue.Empty:
        await message.reply("–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–≤—Ç–æ—Ä–∏—Ç–µ –ø–æ–ø—ã—Ç–∫—É –ø–æ–∑–∂–µ.", parse_mode="Markdown")
    finally:
        request_queue.task_done()

# –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞
if __name__ == '__main__':
    executor.start_polling(dp, skip_updates=True)
